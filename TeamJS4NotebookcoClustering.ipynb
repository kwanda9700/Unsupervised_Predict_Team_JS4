{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Description\n",
    "\n",
    "\n",
    "In today’s technology driven world, recommender systems are socially and economically critical for ensuring that individuals can make appropriate choices surrounding the content they engage with on a daily basis. One application where this is especially true surrounds movie content recommendations; where intelligent algorithms can help viewers find great titles from tens of thousands of options.\n",
    "\n",
    "With this context, EDSA is challenging you to construct a recommendation algorithm based on content or collaborative filtering, capable of accurately predicting how a user will rate a movie they have not yet viewed based on their historical preferences.\n",
    "\n",
    "Providing an accurate and robust solution to this challenge has immense economic potential, with users of the system being exposed to content they would like to view or purchase - generating revenue and platform affinity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation metric for this competition is Root Mean Square Error. Root Mean Square Error  is commonly used in regression analysis and forecasting, and measures the standard deviation of the residuals arising between predicted and actual observed values for a modelling process. For our task of generating user movie ratings via recommendation algorithms, the the formula is given by:\n",
    "\n",
    "Where \\\\( \\hat{R} \\\\) is the total number of recommendations generated for users and movies, with \\\\( r_{ui} \\\\) and \\\\( \\hat{r}_{ui} \\\\) being the true and predicted ratings for user \\\\( u \\\\) watching movie \\\\( i \\\\) respectively.\n",
    "\n",
    "### Submission Format\n",
    "For every author in the dataset, submission files should contain two columns: Id and rating. 'Id' is a concatenation of the userID and movieID given in the test file using an '_' character. 'rating' is the predicted rating for a given user-movie pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing packages\n",
    "Please download all relevant packages in. There is no terminal so you will pip install everything.\n",
    "\n",
    "You can find a list of recommended install from the Intro to Recommender sysytem notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-07-15T18:12:21.352858Z",
     "iopub.status.busy": "2021-07-15T18:12:21.352395Z",
     "iopub.status.idle": "2021-07-15T18:12:21.368727Z",
     "shell.execute_reply": "2021-07-15T18:12:21.367588Z",
     "shell.execute_reply.started": "2021-07-15T18:12:21.352756Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing packages\n",
    "Please download all relevant packages in. There is no terminal so you will pip install everything.\n",
    "\n",
    "You can find a list of recommended install from the Intro to Recommender sysytem notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T18:12:22.432498Z",
     "iopub.status.busy": "2021-07-15T18:12:22.432096Z",
     "iopub.status.idle": "2021-07-15T18:12:23.689552Z",
     "shell.execute_reply": "2021-07-15T18:12:23.688638Z",
     "shell.execute_reply.started": "2021-07-15T18:12:22.432463Z"
    }
   },
   "outputs": [],
   "source": [
    "# Install packages here\n",
    "# Packages for data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy as sp\n",
    "\n",
    "# importing the libraries\n",
    "\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Packages for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "\n",
    "# Packages for modeling\n",
    "from surprise import NormalPredictor\n",
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "from surprise import KNNBasic, KNNWithMeans, KNNWithZScore, KNNBaseline\n",
    "from surprise import BaselineOnly, SlopeOne, CoClustering\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import SVD\n",
    "from surprise import SVDpp\n",
    "from surprise import NMF\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.model_selection import GridSearchCV\n",
    "import heapq\n",
    "\n",
    "# Packages for model evaluation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from time import time\n",
    "\n",
    "# Package to suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Packages for saving models\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T18:12:23.691277Z",
     "iopub.status.busy": "2021-07-15T18:12:23.690838Z",
     "iopub.status.idle": "2021-07-15T18:12:49.516495Z",
     "shell.execute_reply": "2021-07-15T18:12:49.51515Z",
     "shell.execute_reply.started": "2021-07-15T18:12:23.691246Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sample_submission = pd.read_csv('/kaggle/input/edsa-movie-recommendation-challenge/sample_submission.csv')\n",
    "df_movies = pd.read_csv('/kaggle/input/edsa-movie-recommendation-challenge/movies.csv')\n",
    "df_imdb = pd.read_csv('/kaggle/input/edsa-movie-recommendation-challenge/imdb_data.csv')\n",
    "df_genome_scores = pd.read_csv('/kaggle/input/edsa-movie-recommendation-challenge/genome_scores.csv')\n",
    "df_genome_tags = pd.read_csv('/kaggle/input/edsa-movie-recommendation-challenge/genome_tags.csv')\n",
    "df_train = pd.read_csv('/kaggle/input/edsa-movie-recommendation-challenge/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/edsa-movie-recommendation-challenge/test.csv')\n",
    "df_tags = pd.read_csv('/kaggle/input/edsa-movie-recommendation-challenge/tags.csv')\n",
    "df_links = pd.read_csv('/kaggle/input/edsa-movie-recommendation-challenge/links.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview¶\n",
    "Let's first look at the shape (number of entries/rows and columns) of the datasets in order to have a general overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T18:12:49.518595Z",
     "iopub.status.busy": "2021-07-15T18:12:49.518247Z",
     "iopub.status.idle": "2021-07-15T18:12:49.564445Z",
     "shell.execute_reply": "2021-07-15T18:12:49.563295Z",
     "shell.execute_reply.started": "2021-07-15T18:12:49.518562Z"
    }
   },
   "outputs": [],
   "source": [
    "# Declaring a list that contains the names of the dataframes\n",
    "dfs = [df_train, df_test, df_genome_scores, df_genome_tags, df_imdb, df_links, df_movies, df_tags]\n",
    "# Create a list of the names of the imported datasets\n",
    "df_names = ['train', 'test', 'genome_scores', 'genome_tags',\n",
    "            'imdb_data', 'links', 'movies', 'tags']\n",
    "dfs_dict = {}  # declaring an empty dictionary\n",
    "for name, data in zip(df_names, dfs):  # iterate over the list and dictionary\n",
    "    dfs_dict[name] = [data.shape[0], data.shape[1]]\n",
    "    df_prop = pd.DataFrame(dfs_dict,\n",
    "                          index=['rows', 'columns']).transpose()\n",
    "df_properties = df_prop.sort_values(by='rows', ascending=False)\n",
    "\n",
    "df_properties  # view the final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T18:12:49.566494Z",
     "iopub.status.busy": "2021-07-15T18:12:49.566136Z",
     "iopub.status.idle": "2021-07-15T18:12:49.578071Z",
     "shell.execute_reply": "2021-07-15T18:12:49.577281Z",
     "shell.execute_reply.started": "2021-07-15T18:12:49.56646Z"
    }
   },
   "outputs": [],
   "source": [
    "df_movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T18:12:49.579793Z",
     "iopub.status.busy": "2021-07-15T18:12:49.579419Z",
     "iopub.status.idle": "2021-07-15T18:12:49.602228Z",
     "shell.execute_reply": "2021-07-15T18:12:49.601356Z",
     "shell.execute_reply.started": "2021-07-15T18:12:49.579753Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T18:12:49.604159Z",
     "iopub.status.busy": "2021-07-15T18:12:49.603818Z",
     "iopub.status.idle": "2021-07-15T18:12:49.628299Z",
     "shell.execute_reply": "2021-07-15T18:12:49.627148Z",
     "shell.execute_reply.started": "2021-07-15T18:12:49.604126Z"
    }
   },
   "outputs": [],
   "source": [
    "df_imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T18:12:49.631023Z",
     "iopub.status.busy": "2021-07-15T18:12:49.63067Z",
     "iopub.status.idle": "2021-07-15T18:12:49.647673Z",
     "shell.execute_reply": "2021-07-15T18:12:49.646715Z",
     "shell.execute_reply.started": "2021-07-15T18:12:49.63099Z"
    }
   },
   "outputs": [],
   "source": [
    "df_genome_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T18:12:49.649851Z",
     "iopub.status.busy": "2021-07-15T18:12:49.649357Z",
     "iopub.status.idle": "2021-07-15T18:12:49.668197Z",
     "shell.execute_reply": "2021-07-15T18:12:49.666826Z",
     "shell.execute_reply.started": "2021-07-15T18:12:49.6498Z"
    }
   },
   "outputs": [],
   "source": [
    "df_genome_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T18:12:49.670087Z",
     "iopub.status.busy": "2021-07-15T18:12:49.669736Z",
     "iopub.status.idle": "2021-07-15T18:12:49.689448Z",
     "shell.execute_reply": "2021-07-15T18:12:49.688352Z",
     "shell.execute_reply.started": "2021-07-15T18:12:49.670055Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T18:12:49.691551Z",
     "iopub.status.busy": "2021-07-15T18:12:49.691134Z",
     "iopub.status.idle": "2021-07-15T18:12:49.70684Z",
     "shell.execute_reply": "2021-07-15T18:12:49.70567Z",
     "shell.execute_reply.started": "2021-07-15T18:12:49.691511Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T18:12:49.708994Z",
     "iopub.status.busy": "2021-07-15T18:12:49.708604Z",
     "iopub.status.idle": "2021-07-15T18:12:49.728424Z",
     "shell.execute_reply": "2021-07-15T18:12:49.727318Z",
     "shell.execute_reply.started": "2021-07-15T18:12:49.708958Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T18:12:49.730266Z",
     "iopub.status.busy": "2021-07-15T18:12:49.729893Z",
     "iopub.status.idle": "2021-07-15T18:12:49.749035Z",
     "shell.execute_reply": "2021-07-15T18:12:49.748081Z",
     "shell.execute_reply.started": "2021-07-15T18:12:49.730234Z"
    }
   },
   "outputs": [],
   "source": [
    "df_links.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of imdb_data\n",
    "The plot below is a visual representation of the different columns in the imdb_data dataset with their percentage of missing values.\n",
    "\n",
    "There is a high number of movies without budget, director or title cast. Such high proportions of missing data largely disqualifies this particular set from our current modelling task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T18:12:49.751009Z",
     "iopub.status.busy": "2021-07-15T18:12:49.750466Z",
     "iopub.status.idle": "2021-07-15T18:12:50.063708Z",
     "shell.execute_reply": "2021-07-15T18:12:50.062557Z",
     "shell.execute_reply.started": "2021-07-15T18:12:49.750972Z"
    }
   },
   "outputs": [],
   "source": [
    "# The percentage of each column of missing values\n",
    "total = df_imdb.isnull().sum().sort_values(ascending=False)\n",
    "percent_1 = df_imdb.isnull().sum()/df_imdb.isnull().count()*100\n",
    "percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent_2],\n",
    "                         axis=1, keys=['Total', '(%) missing'])\n",
    "missing_data['(%) missing'].plot(kind='barh')\n",
    "plt.xlabel('(%) Missing Values')\n",
    "plt.ylabel('Columns with Missing Values')\n",
    "plt.title('Percentage of Missing Values per Column')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T18:12:50.065451Z",
     "iopub.status.busy": "2021-07-15T18:12:50.065105Z",
     "iopub.status.idle": "2021-07-15T18:12:56.466859Z",
     "shell.execute_reply": "2021-07-15T18:12:56.465733Z",
     "shell.execute_reply.started": "2021-07-15T18:12:50.065419Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for unique users and movieId's in the train dataset\n",
    "users = len(df_train.userId.unique())\n",
    "items = len(df_train.movieId.unique())\n",
    "print('There are {} unique users and {}\\\n",
    " unique movies train dataset with {} duplicated entries'.format(users, items, df_train[df_train.duplicated()].count().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Merging of Datasets\n",
    "Now that we have a basic understanding of the data we are working with, we merge the sets below for more in depth analysis in the EDA section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T18:12:56.468617Z",
     "iopub.status.busy": "2021-07-15T18:12:56.468311Z",
     "iopub.status.idle": "2021-07-15T18:13:03.692453Z",
     "shell.execute_reply": "2021-07-15T18:13:03.691301Z",
     "shell.execute_reply.started": "2021-07-15T18:12:56.468586Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combining both train and movies datasets by using movieId\n",
    "# as the matching column between both datasets\n",
    "train_movies_df = pd.merge(df_train,\n",
    "                           df_movies,\n",
    "                           how='left',\n",
    "                           on='movieId')\n",
    "\n",
    "# Combining all the observations in movies_metadata_df with imdb_data\n",
    "# using movieId as the matching column between both dataframes\n",
    "movies_metadata_df = pd.merge(train_movies_df,\n",
    "                              df_imdb,\n",
    "                              how='left',\n",
    "                              on='movieId')\n",
    "\n",
    "movies_metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T18:13:03.694094Z",
     "iopub.status.busy": "2021-07-15T18:13:03.693784Z",
     "iopub.status.idle": "2021-07-15T18:13:08.555609Z",
     "shell.execute_reply": "2021-07-15T18:13:08.554585Z",
     "shell.execute_reply.started": "2021-07-15T18:13:03.694064Z"
    }
   },
   "outputs": [],
   "source": [
    "movies_ranking = movies_metadata_df[['title','rating']].groupby('title').mean().sort_values('rating', ascending=False)\n",
    "movies_ranking.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "Discovery phase and data understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the timestamp column because we don't need it\n",
    "df_train = df_train.drop('timestamp', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ratings = pd.merge(df_movies, df_train)\n",
    "movie_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics\n",
    "The descriptive statistics of the movies dataset below does not provide any useful information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_movies.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualizations\n",
    "Distribution of rating score in ratings df\n",
    "Showing the distribution of ratings given in the ratings dataset. In the plot below, we see that the rating scale ranges from 0.5 to 5.0 with increments of 0.5. The most prevalent ratings given are 3.0, and 4.0 with 5.0 coming in third. We also see that people were less likely to give low ratings as evidenced by the low number of movies rated between 0.5 and 2.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize = (14,8))\n",
    "sns.countplot(x = df_train.rating)\n",
    "plt.suptitle('Frequency Distribution of Rating Scores', fontsize = 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of User Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'Average rating in the dataset: {np.mean(df_train[\"rating\"])}')\n",
    "\n",
    "with sns.axes_style('white'):\n",
    "    g = sns.factorplot(\"rating\", data=df_train, aspect=2.5, kind='count')\n",
    "    g.set_ylabels(\"Total number of ratings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Rating per Genre\n",
    "The genres with the highest average ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subsetting the genres and rating columns from movie_ratings\n",
    "genre_ratingdf = movie_ratings.loc[:,['genres', 'rating']]\n",
    "\n",
    "# Unravel the genre columns\n",
    "genre_ratingdf = genre_ratingdf.set_index(genre_ratingdf.columns.drop('genres',1).tolist()).genres.str.split('|', expand=True).stack().reset_index().rename(columns={0:'genres'}).loc[:, genre_ratingdf.columns]\n",
    "\n",
    "# group by genres and their mean rating score and resetting the index\n",
    "genre_mean_rating = pd.DataFrame(genre_ratingdf.groupby('genres')['rating'].mean()).reset_index()\n",
    "\n",
    "# plotting the figure\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "sns.barplot(x = 'genres', y = 'rating', data = genre_mean_rating)\n",
    "\n",
    "ax.tick_params(axis='x', labelsize= 15, rotation=90)\n",
    "ax.set_xlabel('Genres', fontsize= 20)\n",
    "ax.set_ylabel('Average Rating Score',fontsize= 20)\n",
    "plt.suptitle('Average Rating per Genre', fontsize= 25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Observation\n",
    "\n",
    "The bar graph above shows that most movies have a rating of 4 followed by 3 while the least rated movies were rated 0.5 and 1.5. The mean rating is around 3.5 revealing that users tend to give higher ratings to movies in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_ranking['No_of_ratings'] = movies_metadata_df.groupby('title')['rating'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_ranking.sort_values(by=['No_of_ratings', 'rating'],\n",
    "                          ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot size\n",
    "sns.set(rc={'figure.figsize':(12,9)})\n",
    "\n",
    "# Plot Number of rating for every rating category.\n",
    "sns.scatterplot(x='rating', y='No_of_ratings', data=movies_ranking)\n",
    "plt.title('Number of ratings per average rating per movie')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Number of ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above scatterplot shows that there is a strong correlation between the number of ratings a rating-category contains and the rating category, i.e. movies that have more ratings (views) strongly tend to also have higher average ratings. This supports the previously established notion that users tend to give higher ratings in general. The plot below similarly shows that even movies with more than one hundred views (ratings) the average rating stays consistent around 3.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average rating of movies in the dataset\n",
    "avg_rating = df_train.groupby('movieId')['rating'].mean()\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(12,10))\n",
    "avg_rating.plot(kind='hist')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Movie Rating')\n",
    "plt.title('Average ratings of movies with 100 or more viewers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we look at average rating for individual movie directors. From the resulting dataframe it is clear that certian directors like Quentin Tarantino, Lilly Wachowski, and Stephen Kind have both higher-than-average average ratings AND higher numbers of ratings (views)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_director = pd.DataFrame(movies_metadata_df.groupby('director')['rating'].mean().\n",
    "                             sort_values(ascending=False))\n",
    "best_director['No_of_ratings'] = movies_metadata_df.groupby('director')['rating'].count()\n",
    "best_director.sort_values(by=['No_of_ratings', 'rating'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below shows the distribution of ratings for directors, extending the positive correclation between 'number of ratings' and 'rating' to 'directors' as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot size\n",
    "sns.set(rc={'figure.figsize':(12,9)})\n",
    "\n",
    "sns.scatterplot(x = 'rating', y = 'No_of_ratings', data = best_director).set_title('Number of ratings per average rating per director')\n",
    "plt.xlabel('Ratings')\n",
    "plt.ylabel('Number of Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most common Genres**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe containing only the movieId and genres\n",
    "movies_genres = pd.DataFrame(df_movies[['movieId', 'genres']],\n",
    "                             columns=['movieId', 'genres'])\n",
    "\n",
    "# Split genres seperated by \"|\" and create a list containing the genres allocated to each movie\n",
    "movies_genres.genres = movies_genres.genres.apply(lambda x: x.split('|'))\n",
    "\n",
    "# Create expanded dataframe where each movie-genre combination is in a seperate row\n",
    "movies_genres = pd.DataFrame([(tup.movieId, d) for tup in movies_genres.itertuples() for d in tup.genres],\n",
    "                             columns=['movieId', 'genres'])\n",
    "\n",
    "movies_genres.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the genres from most common to least common\n",
    "plot = plt.figure(figsize=(15, 10))\n",
    "plt.title('Most common genres\\n', fontsize=20)\n",
    "sns.countplot(y=\"genres\", data=movies_genres,\n",
    "              order=movies_genres['genres'].value_counts(ascending=False).index,\n",
    "              palette='Reds_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prepartion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T18:13:08.557159Z",
     "iopub.status.busy": "2021-07-15T18:13:08.556853Z",
     "iopub.status.idle": "2021-07-15T18:14:04.164558Z",
     "shell.execute_reply": "2021-07-15T18:14:04.163485Z",
     "shell.execute_reply.started": "2021-07-15T18:13:08.55713Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a small test dataframe to evaluate our models\n",
    "tests = df_train.copy()\n",
    "\n",
    "tests = tests.head(20000)\n",
    "\n",
    "# Creating the training data\n",
    "reader = Reader(rating_scale=(0.5, 5))\n",
    "test_data = Dataset.load_from_df(tests[['userId','movieId','rating']], reader)\n",
    "\n",
    "# Compute similarities between users using cosine distance\n",
    "sim_options = {\"name\": \"cosine\",\n",
    "               \"user_based\": True}  \n",
    "\n",
    "# Evaluate the model \n",
    "user = KNNWithMeans(sim_options=sim_options)\n",
    "cv = cross_validate(user, test_data, cv=5, measures=['RMSE'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T18:14:04.166288Z",
     "iopub.status.busy": "2021-07-15T18:14:04.165967Z",
     "iopub.status.idle": "2021-07-15T18:14:10.410334Z",
     "shell.execute_reply": "2021-07-15T18:14:10.409282Z",
     "shell.execute_reply.started": "2021-07-15T18:14:04.166254Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute similarities between items using cosine distance\n",
    "sim_options = {\"name\": \"cosine\",\n",
    "               \"user_based\": False}  \n",
    "\n",
    "# Fit the KNNwithmeans algorithm to the training set\n",
    "item_based = KNNWithMeans(sim_options=sim_options)\n",
    "\n",
    "# Evaluate the model \n",
    "cv = cross_validate(item_based, test_data, cv=5, measures=['RMSE'], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling phase\n",
    "\n",
    "Here you can apply the models outline in the Intro to Recommender Notebook. You only need to apply one version \n",
    "be it Content based or Collabrative method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T18:14:10.414201Z",
     "iopub.status.busy": "2021-07-15T18:14:10.413828Z",
     "iopub.status.idle": "2021-07-15T18:14:24.883376Z",
     "shell.execute_reply": "2021-07-15T18:14:24.882179Z",
     "shell.execute_reply.started": "2021-07-15T18:14:10.414152Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading as Surprise dataframe \n",
    "reader = Reader()\n",
    "data = Dataset.load_from_df(df_train[['userId', 'movieId', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T18:14:24.888545Z",
     "iopub.status.busy": "2021-07-15T18:14:24.888022Z",
     "iopub.status.idle": "2021-07-15T18:15:11.006345Z",
     "shell.execute_reply": "2021-07-15T18:15:11.00507Z",
     "shell.execute_reply.started": "2021-07-15T18:14:24.888493Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data split 85/15\n",
    "trainset, testset = train_test_split(data, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T18:15:11.007982Z",
     "iopub.status.busy": "2021-07-15T18:15:11.007676Z",
     "iopub.status.idle": "2021-07-15T18:15:11.011798Z",
     "shell.execute_reply": "2021-07-15T18:15:11.01083Z",
     "shell.execute_reply.started": "2021-07-15T18:15:11.007944Z"
    }
   },
   "outputs": [],
   "source": [
    "co_clust = CoClustering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T18:15:11.013382Z",
     "iopub.status.busy": "2021-07-15T18:15:11.013078Z",
     "iopub.status.idle": "2021-07-15T18:26:29.946191Z",
     "shell.execute_reply": "2021-07-15T18:26:29.945001Z",
     "shell.execute_reply.started": "2021-07-15T18:15:11.013354Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fitting our trainset\n",
    "co_clust.fit(trainset)\n",
    "\n",
    "# Using the 15% testset to make predictions\n",
    "predictions = co_clust.test(testset) \n",
    "predictions\n",
    "\n",
    "test = pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T18:26:29.948249Z",
     "iopub.status.busy": "2021-07-15T18:26:29.947889Z",
     "iopub.status.idle": "2021-07-15T18:26:29.967327Z",
     "shell.execute_reply": "2021-07-15T18:26:29.966062Z",
     "shell.execute_reply.started": "2021-07-15T18:26:29.948211Z"
    }
   },
   "outputs": [],
   "source": [
    "# View the head\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T18:27:09.215873Z",
     "iopub.status.busy": "2021-07-15T18:27:09.215315Z",
     "iopub.status.idle": "2021-07-15T18:36:04.542906Z",
     "shell.execute_reply": "2021-07-15T18:36:04.542116Z",
     "shell.execute_reply.started": "2021-07-15T18:27:09.21584Z"
    }
   },
   "outputs": [],
   "source": [
    "# We are trying to predict ratings for every userId / movieId pair, we implement the below list comprehension to achieve this.\n",
    "ratings_predictions=[co_clust.predict(row.userId, row.movieId) for _,row in df_test.iterrows()]\n",
    "ratings_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T18:36:04.54476Z",
     "iopub.status.busy": "2021-07-15T18:36:04.544304Z",
     "iopub.status.idle": "2021-07-15T18:36:13.595001Z",
     "shell.execute_reply": "2021-07-15T18:36:13.59388Z",
     "shell.execute_reply.started": "2021-07-15T18:36:04.544728Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Converting our prediction into a familiar format-Dataframe\n",
    "df_pred=pd.DataFrame(ratings_predictions)\n",
    "df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T18:36:13.597055Z",
     "iopub.status.busy": "2021-07-15T18:36:13.596734Z",
     "iopub.status.idle": "2021-07-15T18:36:13.918585Z",
     "shell.execute_reply": "2021-07-15T18:36:13.917497Z",
     "shell.execute_reply.started": "2021-07-15T18:36:13.597025Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Renaming our predictions to original names\n",
    "df_pred=df_pred.rename(columns={'uid':'userId', 'iid':'movieId','est':'rating'})\n",
    "df_pred.drop(['r_ui','details'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T18:36:13.920539Z",
     "iopub.status.busy": "2021-07-15T18:36:13.920251Z",
     "iopub.status.idle": "2021-07-15T18:36:13.932158Z",
     "shell.execute_reply": "2021-07-15T18:36:13.931206Z",
     "shell.execute_reply.started": "2021-07-15T18:36:13.920511Z"
    }
   },
   "outputs": [],
   "source": [
    "# Snippet of our ratings\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating userId/movieId into a single Id column.(code has to be run twice to get desired outcome)\n",
    "df_pred['Id']=df_pred.apply(lambda x:'%s_%s' % (x['userId'],x['movieId']),axis=1)\n",
    "df_pred['Id']=df_pred.apply(lambda x:'%s_%s' % (x['userId'],x['movieId']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the two features from the dataset userId and movieId\n",
    "df_pred.drop(['userId', 'movieId'], inplace=True, axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the prediction dataset\n",
    "df_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-output": true
   },
   "source": [
    "Prepare Submission File\n",
    "We make submissions in CSV files. Your submissions usually have two columns: an ID column and a prediction column. The ID field comes from the test data (keeping whatever name the ID field had in that data, which for the data is the string 'Id'). The prediction column will use the name of the target field.\n",
    "\n",
    "We will create a DataFrame with this data, and then use the dataframe's to_csv method to write our submission file. Explicitly include the argument index=False to prevent pandas from adding another column in our csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission final csv. file\n",
    "df_pred.to_csv(\"coClustering.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Make Submission\n",
    "Hit the blue Publish button at the top of your notebook screen. It will take some time for your kernel to run. When it has finished your navigation bar at the top of the screen will have a tab for Output. This only shows up if you have written an output file (like we did in the Prepare Submission File step)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example below of how the output would look once published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize list of lists\n",
    "data = [['tom', 10], ['nick', 15], ['juli', 14]]\n",
    "  \n",
    "# Create the pandas DataFrame\n",
    "df = pd.DataFrame(data, columns = ['Name', 'Age'])\n",
    "  \n",
    "# print dataframe.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('my_test_output.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborators\n",
    "1. Kwanda Silekwa\n",
    "2. Nomfundo Manyisa\n",
    "3. Sihle Riti\n",
    "4. Thanyani Khedzi\n",
    "5. Thembinkosi Malefo\n",
    "6. Ofentse Makeketlane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/exploring-movie-data-with-interactive-visualizations-c22e8ce5f663"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://in.springboard.com/blog/recommender-system-with-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://asdkazmi.medium.com/ai-movies-recommendation-system-with-clustering-based-k-means-algorithm-f04467e02fcd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Wonuabimbola/movie-recommendation-system/blob/master/movie_rec_system.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/unsupervised-classification-project-building-a-movie-recommender-with-clustering-analysis-and-4bab0738efe6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/dibyawantrivedi/movie-recommendation-system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/how-to-build-a-simple-recommender-system-in-python-375093c3fb7d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
